# 🤖 GenAI Smart Research Assistant (Offline)

This project is a **fully local, offline GenAI assistant** built using **Streamlit**, **LangChain**, and **Ollama**. It helps users interact with PDF documents by answering questions, generating logic-based challenges, and evaluating user responses — all without relying on the OpenAI API or internet access.

---

## 🧠 Features

- 📄 **PDF/Text Upload**  
  Upload any PDF or `.txt` file for instant processing.

- 📝 **Auto Summarization**  
  Generates a short summary (≤150 words) using local LLMs like `phi3`.

- 💬 **Ask Anything**  
  Users can ask free-form questions based on the uploaded document.

- 🧠 **Challenge Me Mode**  
  Automatically generates 3 logic-based or comprehension questions from the file.

- ✅ **Answer Evaluation**  
  User answers are evaluated with feedback in ≤3 lines using local model reasoning.

- 🔒 **Runs 100% Locally**  
  No API key or internet required — uses Ollama + LangChain for inference.

---

## 🏗️ Project Structure

genai_assistant/
├── app.py # Streamlit frontend
├── requirements.txt # Python dependencies
├── .gitignore
├── utils/ # Core logic modules
│ ├── pdf_parser.py
│ ├── summarizer.py
│ ├── qa.py
│ └── question_generator.py

yaml
Copy
Edit

---

## 🛠️ Setup Instructions (Run Locally)

### ✅ Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com) installed (for local LLMs)
- Git (optional but helpful)

---

### 🧱 Step-by-Step

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/genai-smart-assistant.git
   cd genai-smart-assistant
(Optional) Create virtual environment

bash
Copy
Edit
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # macOS/Linux
Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
Pull local LLM

bash
Copy
Edit
ollama pull phi3
Run the app

bash
Copy
Edit
streamlit run app.py
⚙️ Architecture & Reasoning Flow
markdown
Copy
Edit
1. User uploads PDF → parsed using PyMuPDF
2. Summary generated using LangChain + Ollama model (phi3/mistral)
3. Ask Anything mode:
   - User types question
   - Prompt + doc context sent to model
4. Challenge Me mode:
   - 3 logic questions generated by LLM
   - User answers are submitted
   - Assistant evaluates and gives feedback (2–3 lines)
🧪 Example Use Cases
📚 Analyze academic PDFs quickly

🧠 Challenge yourself with logic questions from reports

📝 Prepare for viva/exams using your own notes

🔒 Use AI safely without sharing your data online

🛡️ Security & Privacy
✅ All data stays on your device (no cloud)

❌ No API keys needed

🔐 No third-party tracking

🙋‍♂️ Author
Developed by Anshul Kumar
Internship Project Submission for EZ
