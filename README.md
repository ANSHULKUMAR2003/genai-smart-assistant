# ğŸ¤– GenAI Smart Research Assistant (Offline)

This project is a **fully local, offline GenAI assistant** built using **Streamlit**, **LangChain**, and **Ollama**. It helps users interact with PDF documents by answering questions, generating logic-based challenges, and evaluating user responses â€” all without relying on the OpenAI API or internet access.

---

## ğŸ§  Features

- ğŸ“„ **PDF/Text Upload**  
  Upload any PDF or `.txt` file for instant processing.

- ğŸ“ **Auto Summarization**  
  Generates a short summary (â‰¤150 words) using local LLMs like `phi3`.

- ğŸ’¬ **Ask Anything**  
  Users can ask free-form questions based on the uploaded document.

- ğŸ§  **Challenge Me Mode**  
  Automatically generates 3 logic-based or comprehension questions from the file.

- âœ… **Answer Evaluation**  
  User answers are evaluated with feedback in â‰¤3 lines using local model reasoning.

- ğŸ”’ **Runs 100% Locally**  
  No API key or internet required â€” uses Ollama + LangChain for inference.

---

## ğŸ—ï¸ Project Structure
genai_assistant/
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ utils/ # Core logic modules
â”‚ â”œâ”€â”€ pdf_parser.py
â”‚ â”œâ”€â”€ summarizer.py
â”‚ â”œâ”€â”€ qa.py
â”‚ â””â”€â”€ question_generator.py


---

## ğŸ› ï¸ Setup Instructions (Run Locally)

### âœ… Prerequisites
- Python 3.10+
- [Ollama](https://ollama.com) installed (for running local LLMs)
- Git installed (optional but recommended)

### ğŸ§± Step-by-Step

1. **Clone the repository**  
   ```bash
   git clone https://github.com/YOUR_USERNAME/genai-smart-assistant.git
   cd genai-smart-assistant
2. **Install dependencies**
     pip install -r requirements.txt
   
3. **Pull your local LLM**
     ollama pull phi3

4.  **Run the app**
     streamlit run app.py
    
**âš™ï¸ Architecture & Reasoning Flow**
    1. User uploads PDF â†’ Parsed using PyMuPDF
    2. Summary is generated via LLM prompt (LangChain + Ollama)
    3. In Ask Mode:
       - User inputs question
       - Prompt sent to model with document context
    4. In Challenge Mode:
       - Model generates 3 logic questions
       - User answers each
   - Each answer is evaluated based on document & instructions

**ğŸ§ª Example Use Cases**
Quickly analyze academic PDFs

Challenge yourself with logic questions from reports

Prepare for viva or exams using your own notes

Run safe AI assistants without sharing documents online


**ğŸ›¡ï¸ Security & Privacy**
    âœ… All data is processed locally

    âŒ No internet or cloud used

    ğŸ”’ No API keys or tracking


Developed by Anshul Kumar
Internship Project Submission for EZ




