# ğŸ¤– GenAI Smart Research Assistant (Offline)

This project is a **fully local, offline GenAI assistant** built using **Streamlit**, **LangChain**, and **Ollama**. It helps users interact with PDF documents by answering questions, generating logic-based challenges, and evaluating user responses â€” all without relying on the OpenAI API or internet access.

---

## ğŸ§  Features

- ğŸ“„ **PDF/Text Upload**  
  Upload any PDF or `.txt` file for instant processing.

- ğŸ“ **Auto Summarization**  
  Generates a short summary (â‰¤150 words) using local LLMs like `phi3`.

- ğŸ’¬ **Ask Anything**  
  Users can ask free-form questions based on the uploaded document.

- ğŸ§  **Challenge Me Mode**  
  Automatically generates 3 logic-based or comprehension questions from the file.

- âœ… **Answer Evaluation**  
  User answers are evaluated with feedback in â‰¤3 lines using local model reasoning.

- ğŸ”’ **Runs 100% Locally**  
  No API key or internet required â€” uses Ollama + LangChain for inference.

---

## ğŸ—ï¸ Project Structure

genai_assistant/
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ utils/ # Core logic modules
â”‚ â”œâ”€â”€ pdf_parser.py
â”‚ â”œâ”€â”€ summarizer.py
â”‚ â”œâ”€â”€ qa.py
â”‚ â””â”€â”€ question_generator.py

yaml
Copy
Edit

---

## ğŸ› ï¸ Setup Instructions (Run Locally)

### âœ… Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com) installed (for local LLMs)
- Git (optional but helpful)

---

### ğŸ§± Step-by-Step

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/genai-smart-assistant.git
   cd genai-smart-assistant
(Optional) Create virtual environment

bash
Copy
Edit
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # macOS/Linux
Install dependencies

bash
Copy
Edit
pip install -r requirements.txt
Pull local LLM

bash
Copy
Edit
ollama pull phi3
Run the app

bash
Copy
Edit
streamlit run app.py
âš™ï¸ Architecture & Reasoning Flow
markdown
Copy
Edit
1. User uploads PDF â†’ parsed using PyMuPDF
2. Summary generated using LangChain + Ollama model (phi3/mistral)
3. Ask Anything mode:
   - User types question
   - Prompt + doc context sent to model
4. Challenge Me mode:
   - 3 logic questions generated by LLM
   - User answers are submitted
   - Assistant evaluates and gives feedback (2â€“3 lines)
ğŸ§ª Example Use Cases
ğŸ“š Analyze academic PDFs quickly

ğŸ§  Challenge yourself with logic questions from reports

ğŸ“ Prepare for viva/exams using your own notes

ğŸ”’ Use AI safely without sharing your data online

ğŸ›¡ï¸ Security & Privacy
âœ… All data stays on your device (no cloud)

âŒ No API keys needed

ğŸ” No third-party tracking

ğŸ™‹â€â™‚ï¸ Author
Developed by Anshul Kumar
Internship Project Submission for EZ
